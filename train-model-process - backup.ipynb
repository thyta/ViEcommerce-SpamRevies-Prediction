{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install Keras-Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyvi.ViTokenizer import ViTokenizer #vietnamese tokenizer\n",
    "\n",
    "import re #regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from keras.preprocessing import text, sequence, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text, sequence \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Conv2D, MaxPool2D, Bidirectional, LSTM, GRU, concatenate, GlobalMaxPooling1D, GlobalAveragePooling1D, SpatialDropout1D, Reshape, Dropout, Concatenate\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, Flatten, Conv2D, MaxPool2D, Bidirectional, LSTM, GRU, concatenate, GlobalMaxPooling1D, GlobalAveragePooling1D, SpatialDropout1D, Reshape, Flatten, Dropout, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# from tensorflow.keras.models import Sequential \n",
    "\n",
    "from tensorflow.keras import layers, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATASET = os.path.join('dataset')\n",
    "\n",
    "PATH_TRAIN = os.path.join(DIR_DATASET, 'core-data\\\\train.csv')\n",
    "PATH_DEV = os.path.join(DIR_DATASET, 'core-data\\dev.csv')\n",
    "PATH_TEST = os.path.join(DIR_DATASET, 'core-data\\\\test.csv')\n",
    "\n",
    "EMBEDDING_PATH = os.path.join(DIR_DATASET, 'cc.vi.300.vec')\n",
    "TOKENIZER_PATH = os.path.join(DIR_DATASET, 'tokenizer.pickle')\n",
    "VISTOPWORDS_PATH = os.path.join(DIR_DATASET, \"vietnamese-stopwords.txt\")\n",
    "\n",
    "MODEL_DIR = 'model'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(PATH_TRAIN)\n",
    "\n",
    "data_dev = pd.read_csv(PATH_DEV)\n",
    "\n",
    "data_test = pd.read_csv(PATH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VISTOPWORDS_PATH, \"r\", encoding=\"utf-8\") as ins:\n",
    "    stopwords = []\n",
    "    for line in ins:\n",
    "        dd = line.strip('\\n')\n",
    "        stopwords.append(dd)\n",
    "    stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv data\n",
    "data_train = pd.read_csv(PATH_TRAIN)\n",
    "data_dev = pd.read_csv(PATH_DEV)\n",
    "data_test = pd.read_csv(PATH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['link', 'rating', 'comment', 'categories', 'category', 'product_name',\n",
       "       'description', 'num_sold', 'num_review', 'label', 'spam_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=data_train.drop(['link', 'rating', 'categories', 'category', 'product_name',\n",
    "       'description', 'num_sold', 'num_review', 'spam_label'], axis=1)\n",
    "data_dev=data_dev.drop(['link', 'rating', 'categories', 'category', 'product_name',\n",
    "       'description', 'num_sold', 'num_review', 'spam_label'], axis=1)\n",
    "data_test=data_test.drop(['link', 'rating', 'categories', 'category', 'product_name',\n",
    "       'description', 'num_sold', 'num_review', 'spam_label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sản phẩm đúng quảng cáo. giao hàng hơi chậm nh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mình nhận hàng ngày hôm qua, tiki giao hàng đú...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Máy giặt quá tốt, chỉ có 5 tr được cái máy giặ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chất liệu vải tốt mát nói chung là chất lượng ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tủ tốt giá tốt nhiều hơn so với các trung tâm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mọi người dưới 56kg nên mua sz S thoii nha. Fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>giao nhanh đóng hàng cẩn thận ăn ngon lắm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10 điểm. Giao hàng nhanh. Cầu hình cao màn hìn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bé nhà mình tiêu thụ loại này tốt!! Tuy nhiên ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ỏ Bến Tre mà vừa đặt hàng hôm qua nay nhận dc ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label\n",
       "0  sản phẩm đúng quảng cáo. giao hàng hơi chậm nh...      0\n",
       "1  Mình nhận hàng ngày hôm qua, tiki giao hàng đú...      0\n",
       "2  Máy giặt quá tốt, chỉ có 5 tr được cái máy giặ...      0\n",
       "3  chất liệu vải tốt mát nói chung là chất lượng ...      0\n",
       "4   Tủ tốt giá tốt nhiều hơn so với các trung tâm...      0\n",
       "5  Mọi người dưới 56kg nên mua sz S thoii nha. Fo...      1\n",
       "6          giao nhanh đóng hàng cẩn thận ăn ngon lắm      0\n",
       "7  10 điểm. Giao hàng nhanh. Cầu hình cao màn hìn...      0\n",
       "8  Bé nhà mình tiêu thụ loại này tốt!! Tuy nhiên ...      0\n",
       "9  Ỏ Bến Tre mà vừa đặt hàng hôm qua nay nhận dc ...      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train's duplicates:  8 \n",
      "dev's duplicates:  0 \n",
      "test's duplicates:  3\n"
     ]
    }
   ],
   "source": [
    "dup_train = data_train.duplicated().sum()\n",
    "dup_dev = data_dev.duplicated().sum()\n",
    "dup_test = data_test.duplicated().sum()\n",
    "\n",
    "print(\"train's duplicates: \", dup_train,\n",
    "    \"\\ndev's duplicates: \", dup_dev, \"\\ntest's duplicates: \", dup_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=data_train.drop_duplicates(keep='first')\n",
    "data_dev=data_dev.drop_duplicates(keep='first')\n",
    "data_test=data_test.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train's duplicates:  0 \n",
      "dev's duplicates:  0 \n",
      "test's duplicates:  0\n"
     ]
    }
   ],
   "source": [
    "dup_train = data_train.duplicated().sum()\n",
    "dup_dev = data_dev.duplicated().sum()\n",
    "dup_test = data_test.duplicated().sum()\n",
    "\n",
    "print(\"train's duplicates: \", dup_train,\n",
    "    \"\\ndev's duplicates: \", dup_dev, \"\\ntest's duplicates: \", dup_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filder stop word\n",
    "def filter_stop_words(train_sentences, stop_words):\n",
    "    new_sent = [word for word in train_sentences.split() if word not in stop_words]\n",
    "    train_sentences = ' '.join(new_sent)\n",
    "    return train_sentences\n",
    "\n",
    "# remove emoji\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "# declare func and adding some remove\n",
    "def preprocess(text, tokenized = True, lowercased = True):\n",
    "    text = ViTokenizer.tokenize(text) if tokenized else text\n",
    "    text = filter_stop_words(text, stopwords)\n",
    "    text = deEmojify(text)\n",
    "    text = text.lower() if lowercased else text\n",
    "\n",
    "    text = text.strip()\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    text = re.sub('\\s+', ' ', text) \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text\n",
    "\n",
    "def pre_process_features(X, y, tokenized = True, lowercased = True):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = [preprocess(str(p), tokenized = tokenized, lowercased = lowercased) for p in list(X)]\n",
    "    for idx, ele in enumerate(X):\n",
    "        if not ele:\n",
    "            X = np.delete(X, idx)\n",
    "            y = np.delete(y, idx)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocess:  [HN] Hiện tại bên mình vẫn đang cần tuyển vị trí C# developer (.NET framework, C# biết winforms (2-3 năm)) sẵn sàng làm thêm FE hoặc Python, Java nếu được giao. Lương thỏa thuận, có 13-15 tháng lương. Quan tâm ib lấy JD. \n",
      "\n",
      "Before preprocess:   hn hiện_tại tuyển vị_trí c developer net framework c winforms sẵn_sàng fe python java giao lương thỏa_thuận lương quan_tâm ib jd\n"
     ]
    }
   ],
   "source": [
    "text_01 = \"[HN] Hiện tại bên mình vẫn đang cần tuyển vị trí C# developer (.NET framework, C# biết winforms (2-3 năm)) sẵn sàng làm thêm FE hoặc Python, Java nếu được giao. Lương thỏa thuận, có 13-15 tháng lương. Quan tâm ib lấy JD.\"\n",
    "text_02 = preprocess(text_01)\n",
    "print(\"After preprocess: \", text_01, \"\\n\")\n",
    "print(\"Before preprocess: \", text_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sản phẩm đúng quảng cáo. giao hàng hơi chậm nh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mình nhận hàng ngày hôm qua, tiki giao hàng đú...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Máy giặt quá tốt, chỉ có 5 tr được cái máy giặ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chất liệu vải tốt mát nói chung là chất lượng ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tủ tốt giá tốt nhiều hơn so với các trung tâm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14301</th>\n",
       "      <td>dùng okee ko gây nóng máy nhưng chất của ốp om...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14302</th>\n",
       "      <td>Rất tốt, rất đáng tiền, nằm rất êm, lúc mới mu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14303</th>\n",
       "      <td>giao đủ, đóng gói chắc chắn, giao hàng nhanh \\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14304</th>\n",
       "      <td>giao hỏa tốc rẻ và nhanh. Nhưng trước đó mình ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14305</th>\n",
       "      <td>Giao hàng nhanh, giá ổn, sản phẩm lắp sử dụng ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14298 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  label\n",
       "0      sản phẩm đúng quảng cáo. giao hàng hơi chậm nh...      0\n",
       "1      Mình nhận hàng ngày hôm qua, tiki giao hàng đú...      0\n",
       "2      Máy giặt quá tốt, chỉ có 5 tr được cái máy giặ...      0\n",
       "3      chất liệu vải tốt mát nói chung là chất lượng ...      0\n",
       "4       Tủ tốt giá tốt nhiều hơn so với các trung tâm...      0\n",
       "...                                                  ...    ...\n",
       "14301  dùng okee ko gây nóng máy nhưng chất của ốp om...      0\n",
       "14302  Rất tốt, rất đáng tiền, nằm rất êm, lúc mới mu...      0\n",
       "14303  giao đủ, đóng gói chắc chắn, giao hàng nhanh \\...      1\n",
       "14304  giao hỏa tốc rẻ và nhanh. Nhưng trước đó mình ...      0\n",
       "14305  Giao hàng nhanh, giá ổn, sản phẩm lắp sử dụng ...      0\n",
       "\n",
       "[14298 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slit data so that X_train is review and y_train is label\n",
    "X_train = data_train.iloc[:, 0:1]\n",
    "y_train = data_train.iloc[:, 1:2]\n",
    "\n",
    "X_dev = data_train.iloc[:, 0:1]\n",
    "y_dev = data_train.iloc[:, 1:2]\n",
    "\n",
    "X_test = data_train.iloc[:, 0:1]\n",
    "y_test = data_train.iloc[:, 1:2]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14301</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14302</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14303</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14304</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14305</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14298 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "...      ...\n",
       "14301      0\n",
       "14302      0\n",
       "14303      1\n",
       "14304      0\n",
       "14305      0\n",
       "\n",
       "[14298 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_train into narray (train_X) and simular\n",
    "train_X, train_y= pre_process_features(X_train['comment'], y_train['label'], tokenized=True, lowercased = True)\n",
    "dev_X, dev_y= pre_process_features(X_dev['comment'], y_dev['label'], tokenized=True, lowercased = True)\n",
    "test_X, test_y = pre_process_features(X_test['comment'], y_test['label'], tokenized=True, lowercased = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14271,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14271,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECLARE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(EMBEDDING_PATH, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(lower=False, filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TOKENIZER_PATH, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_featues(X, y, tokenizer, is_one_hot_label=True, number_class1=2):\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(X, maxlen=sequence_length)\n",
    "    if is_one_hot_label: \n",
    "        y = to_categorical(y, num_classes=number_class1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "sequence_length = 100\n",
    "maxlen = 100\n",
    "\n",
    "embedding_dim = 300\n",
    "batch_size = 256\n",
    "epochs = 40\n",
    "drop = 0.5\n",
    "\n",
    "# filter_sizes = [2,3,5]\n",
    "# num_filters = 32\n",
    "\n",
    "filter_sizes = [2,3,5]\n",
    "num_filters = 32\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= vocabulary_size:\n",
    "        continue\n",
    "\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "embedding_dim = 300\n",
    "batch_size = 256\n",
    "epochs = 40\n",
    "drop = 0.5\n",
    "\n",
    "filter_sizes = [2,3,5]\n",
    "num_filters = 32\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= vocabulary_size:\n",
    "        continue\n",
    "\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_XX, train_yy = make_featues(train_X, train_y, tokenizer)\n",
    "dev_XX, dev_yy = make_featues(dev_X, dev_y, tokenizer)\n",
    "test_XX, test_yy = make_featues(test_X, test_y, tokenizer, is_one_hot_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14271, 100)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0  268  141    3  710 6622   18\n",
      "  347   49  414  497   13   65   82 6623    4    7  711   37  119 2741\n",
      "  498  831]\n"
     ]
    }
   ],
   "source": [
    "print(train_XX[5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_yy[6, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (14271, 100)\n",
      "<class 'numpy.ndarray'> (14271, 2)\n",
      "<class 'numpy.ndarray'> (14271, 100)\n",
      "<class 'numpy.ndarray'> (14271, 2)\n",
      "<class 'numpy.ndarray'> (14271, 100)\n",
      "<class 'numpy.ndarray'> (14271,)\n"
     ]
    }
   ],
   "source": [
    "print(type(train_XX), train_XX.shape)\n",
    "print(type(train_yy), train_yy.shape)\n",
    "print(type(dev_XX), dev_XX.shape)\n",
    "print(type(dev_yy), dev_yy.shape)\n",
    "print(type(test_XX), test_XX.shape)\n",
    "print(type(test_yy), test_yy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14271, 100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_XX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TextCNN\n",
    "# Add layers\n",
    "# textCNNModel = Sequential()\n",
    "# textCNNModel.add(layers.Embedding(num_words, embedding_dim, input_length=sequence_length))\n",
    "# # textCNNModel.add(layers.SpatialDropout1D(0.2))\n",
    "# textCNNModel.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "# textCNNModel.add(layers.GlobalMaxPooling1D())\n",
    "# textCNNModel.add(layers.Dense(256, activation='relu'))\n",
    "# textCNNModel.add(layers.Dense(1, activation='softmax'))  # Change to match the number of classes\n",
    "# textCNNModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Change loss function\n",
    "# textCNNModel.summary()\n",
    "\n",
    "# textCNNModel = Sequential()\n",
    "# textCNNModel.add(layers.Embedding(num_words, embedding_dim, input_length=sequence_length))\n",
    "# textCNNModel.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "# textCNNModel.add(layers.GlobalMaxPooling1D())\n",
    "# textCNNModel.add(layers.Dense(256, activation='relu'))\n",
    "# textCNNModel.add(layers.Dense(1, activation='sigmoid'))  # Changed to 'sigmoid' activation, 1 neuron\n",
    "# textCNNModel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Changed loss function\n",
    "\n",
    "textCNNModel = Sequential()\n",
    "textCNNModel.add(layers.Embedding(num_words, embedding_dim, input_length=sequence_length))\n",
    "textCNNModel.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "textCNNModel.add(layers.GlobalMaxPooling1D())\n",
    "textCNNModel.add(layers.Dense(256, activation='relu'))\n",
    "textCNNModel.add(layers.Dense(2, activation='softmax'))  # Changed to 2 neurons for two classes\n",
    "textCNNModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Keep loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56/56 - 17s - 295ms/step - accuracy: 0.7795 - loss: 0.4806 - val_accuracy: 0.8729 - val_loss: 0.3166\n",
      "Epoch 2/10\n",
      "56/56 - 12s - 222ms/step - accuracy: 0.8817 - loss: 0.2970 - val_accuracy: 0.9421 - val_loss: 0.1808\n",
      "Epoch 3/10\n",
      "56/56 - 14s - 249ms/step - accuracy: 0.9416 - loss: 0.1600 - val_accuracy: 0.9762 - val_loss: 0.0797\n",
      "Epoch 4/10\n",
      "56/56 - 13s - 232ms/step - accuracy: 0.9730 - loss: 0.0773 - val_accuracy: 0.9881 - val_loss: 0.0384\n",
      "Epoch 5/10\n",
      "56/56 - 12s - 214ms/step - accuracy: 0.9868 - loss: 0.0416 - val_accuracy: 0.9903 - val_loss: 0.0342\n",
      "Epoch 6/10\n",
      "56/56 - 12s - 216ms/step - accuracy: 0.9905 - loss: 0.0271 - val_accuracy: 0.9954 - val_loss: 0.0160\n",
      "Epoch 7/10\n",
      "56/56 - 12s - 210ms/step - accuracy: 0.9941 - loss: 0.0190 - val_accuracy: 0.9963 - val_loss: 0.0120\n",
      "Epoch 8/10\n",
      "56/56 - 11s - 204ms/step - accuracy: 0.9954 - loss: 0.0149 - val_accuracy: 0.9969 - val_loss: 0.0111\n",
      "Epoch 9/10\n",
      "56/56 - 12s - 219ms/step - accuracy: 0.9948 - loss: 0.0159 - val_accuracy: 0.9954 - val_loss: 0.0137\n",
      "Epoch 10/10\n",
      "56/56 - 12s - 213ms/step - accuracy: 0.9959 - loss: 0.0132 - val_accuracy: 0.9972 - val_loss: 0.0087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e98e334450>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for model accuracy\n",
    "textCNNModel.fit( train_XX, train_yy,\n",
    "    epochs=10,\n",
    "    verbose=2,\n",
    "    validation_data=(dev_XX, dev_yy),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14271, 100)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14271, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_binary = train_yy[:, 0]\n",
    "dev_y_binary = dev_yy[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the TEST data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m textCNNModel\u001b[38;5;241m.\u001b[39mevaluate(test_XX, test_yy)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOSS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(accuracy))\n",
      "File \u001b[1;32mE:\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mE:\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:547\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m     )\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m--> 547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    549\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ndim). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    551\u001b[0m     )\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 2)"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the TEST data\n",
    "loss, accuracy = textCNNModel.evaluate(test_XX, test_yy)\n",
    "print(f\"LOSS: {loss}\")\n",
    "print ('Accuracy '+ str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
